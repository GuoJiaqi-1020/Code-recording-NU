{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 3.1\n",
    "Devise three example tasks of your own that fit into the MDP framework, identifying for each its states, actions, and rewards. Make the three examples as different from each other as possible. The framework is abstract and flexible and can be applied in many ways. Stretch its limits in some way in at least one of your examples."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 1. A conventional supervised image classification problem:\n",
    "    States: the image feed to the network.\n",
    "    Actions: the predicted class\n",
    "    Rewards: loss calculated by a certain loss functions\n",
    "    Limitation: the model may be over trained (overfitting)\n",
    "##### 2. Playing cheese\n",
    "    States: the actual situation on the game board\n",
    "    Actions: move pieces on the board in accordance with the rules of the game on your turn\n",
    "    Rewards: +1/-1 for win and loss\n",
    "##### 3. An airconditioner that will adjust the temp to a certain degree\n",
    "    States: the actual temperature in room\n",
    "    Actions: Increase or decrease the temperature based on the current temperature and the target temperature\n",
    "    Rewards: +1/-1 for lower temp and higher temp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 3.3\n",
    "Consider the problem of driving. You could define the actions in terms of the accelerator, steering wheel, and brake, that is, where your body meets the machine. Or you could define them farther out-say, where the rubber meets the road, considering your actions to be tire torques. Or you could define them farther in-say, where your brain meets your body, the actions being muscle twitches to control your limbs. Or you could go to a really high level and say that your actions are your choices of where to drive. What is the right level, the right place to draw the line between agent and environment? On what basis is one location of the line to be preferred over another? Is there any fundamental reason for preferring one location over another, or is it a free choice?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Answer:\n",
    "\n",
    "There is no right level to draw the line between the agent and environment.\n",
    "\n",
    "The driving problem itself is a complex task composed of multiple layers of RL problems. The line needs to be drawn in terms of what we want to achieve.\n",
    "\n",
    "A higher level line, such as where your brain meets your body, lead to less computation efficiency. Low level lines, however, may lead to the loss of some details, like the state change of the direction during driving, resulting in some safety issues."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 3.4\n",
    "Give a table analogous to that in Example 3.3, but for $p(s', r|s, a)$. It should have columns for $s, a, s', r$, and $p(s', r|s, a)$, and a row for every 4-tuple for which $p(s', r|s, a) > 0$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Answer:\n",
    "The rewards have no probability distribution, which indicates that  $p(s'|s, a)=p(s',r|s,a)$\n",
    "\n",
    "|  $s$ |    $a$   | $s'$ |    $r$     | $p(s',r|s,a)$  |\n",
    "|:----:|:--------:|:----:|:----------:|:--------------:|\n",
    "| $high$ |  $search$  | $high$ | $r_{research}$ |     $\\alpha$   |\n",
    "| $high$ |  $search$  |  $low$ | $r_{research}$|    $1-\\alpha$  |\n",
    "|  $low$ |  $search$  | $high$ |     $-3$     |    $1-\\beta$   |\n",
    "|  $low$ |  $search$  |  $low$ | $r_{research}$ |     $\\beta $   |\n",
    "| $high$ |   $wait$   | $high$ |   $r_{wait}$   |    $ 1 $       |\n",
    "| $high$ |   $wait$   |  $low$ |      $-$     |      $ 0 $     |\n",
    "|  $low$ |   $wait$   | $high$ |      $-$     |      $ 0 $     |\n",
    "|  $low$ |   $wait$   |  $low$ |   $r_{wait}$   |       $ 1 $    |\n",
    "|  $low$ | $recharge$ | $high$ |      $0$     |        $ 1 $   |\n",
    "|  $low$ | $recharge$ |  $low$ |      $-$     |         $ 0 $  |"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Investing in Stocks\n",
    "Letâ€™s say you start off at day one, with two stocks $S_1$ and $S_2$ to choose to invest in. The return of stock one is uniformly distributed (as a percentage) between [-1,1]. The return of stock two is uniformly distributed (as a percentage) between [-2,8]. Assume you start off on day one with 100 dollars."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Part 1\n",
    "Write a simulation that runs for one hundred iterations, and randomly invests in stock one with a fifty percent chance, and in stock two with a fifty percent chance. What is your final return on day 100, assuming you start day 1 with 100 dollars? Please attach your code to this assignment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the final return on day 100 is: 325.29248479845916\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def choose_stock():\n",
    "    indices = random.randint(0,1)\n",
    "    if indices == 0:\n",
    "        return stock_s1()\n",
    "    else:\n",
    "        return stock_s2()\n",
    "\n",
    "\n",
    "def stock_s1():\n",
    "    return random.randint(-1,1)/100\n",
    "\n",
    "def stock_s2():\n",
    "    return random.randint(-2,8)/100\n",
    "\n",
    "funding = 100\n",
    "for day in range(100):\n",
    "    funding = funding*(1+choose_stock())\n",
    "print(f\"the final return on day 100 is: {funding}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Part 2\n",
    "In this problem, what is the state variable(s)? What is the action variable(s)? What are the possible states and actions? What are the possible rewards each day?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Answer:\n",
    "    States: which stock we choose\n",
    "    Actions: the principal changes over time\n",
    "    Rewards: the return of the chosen stock\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Part 3\n",
    "If you wanted to maximize your mean return, which stock is better to invest in? Hint: The solution is very short, you could write it in a sentence or two."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Answer:\n",
    "The stock 2 is better to invest in, since it has a higher return expectation $E_{return}$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}