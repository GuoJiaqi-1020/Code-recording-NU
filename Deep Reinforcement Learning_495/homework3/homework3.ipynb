{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment 3\n",
    "Jiaqi Guo    JGR9647"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A Bandit Problem"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Part 1\n",
    "Consider the following situation. You start with a thousand dollars to invest in two different stocks, **Apple** or **Microsoft**, for 100 days. At the beginning of each day, you must pick a stock to invest in. At the end of each day, you observe the return, and decide which stock to invest in the next day. You don’t know what the return will be the next day, but you know that the return (as a proportion) of **Apple** is normally distributed with $\\mu = 0.05$ and $\\sigma^2 = 0.1$. The return of **Microsoft** is normally distributed with $\\mu = 0.1$ and $\\sigma^2 = 0.3$\n",
    "\n",
    "Before doing any analyses, what do you expect to be the best strategy Would you prefer to invest in **Apple** or **Microsoft**? Or would you vary your investment based off of your balance? (There is no right answer here, and grading will only be based off of a thoughtful answer that makes a justification by addressing the probability distributions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Answer:\n",
    "I would choose Microsoft for 100 days because it has a higher $\\mu$ value (higher expectation). And, in theory, we have a 57.2 percent chance of making a positive return with Microsoft stock, compared with 56.3 percent with Apple"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42756607029235294\n",
      "0.4371835305814459\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats\n",
    "\n",
    "# Microsoft\n",
    "print(scipy.stats.norm.cdf(0,loc=0.1, scale=(0.3**0.5)))\n",
    "# Apple\n",
    "print(scipy.stats.norm.cdf(0,loc=0.05, scale=(0.1**0.5)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Part 2\n",
    "Using python or a language of your choice, write a program that simulates your strategy (a 100 day simulation starting with 1000 dollars). Repeat the simulation 100 times and report the mean and variance of your results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The variance of 100 times simulation is: 397479.7292494515\n",
      " The mean of 100 times simulation is: 63.57723012847625\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# np.random.seed(6)\n",
    "return_list = []\n",
    "company_choice = \"Microsoft\"\n",
    "\n",
    "def bandit_100_days(company:str, money: float)-> tuple:\n",
    "    if company =='Microsoft':\n",
    "        return money*max((1+np.random.normal(loc=0.1, scale=0.3**0.5)),0)\n",
    "    else:\n",
    "        return money*max((1+np.random.normal(loc=0.05, scale=0.1**0.5)),0)\n",
    "for _ in range(100):\n",
    "    money = 1000\n",
    "    for day in range(100):\n",
    "        money= bandit_100_days(company_choice, money)\n",
    "    return_list.append(money)\n",
    "\n",
    "print(f' The variance of 100 times simulation is: {np.var(return_list)}')\n",
    "print(f' The mean of 100 times simulation is: {np.mean(return_list)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Part 3\n",
    "One approximation to the problem is known as the $exp3$ algorithm. It initially gives equal weights in deciding between both stocks, and based of results, re-weights the probability of choosing the next stock. An algorithm[1] is shown below, where you may set $\\gamma = 1, T = 100$ (the number of days) and $i = 1$ represents Apple and $i = 2$ represents Microsoft.\n",
    "\n",
    "Implement this algorithm for the problem, and report the final return of the algorithm. How does it compare to your result in Part 2?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The variance of 100 times simulation is: 26483939980.40379\n",
      " The mean of 100 times simulation is: 27887.14463351453\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "reward_history = []\n",
    "\n",
    "def draw(weights):\n",
    "    choice = np.random.uniform(0, sum(weights))\n",
    "    choiceIndex = 0\n",
    "\n",
    "    for weight in weights:\n",
    "        choice -= weight\n",
    "        if choice <= 0:\n",
    "            return choiceIndex\n",
    "        choiceIndex += 1\n",
    "\n",
    "def distr(weights, gamma=0.0):\n",
    "    theSum = float(sum(weights))\n",
    "    return tuple((1.0 - gamma) * (w / theSum) + (gamma / len(weights)) for w in weights)\n",
    "\n",
    "def exp3(numActions, reward, gamma=1, rewardMin = 0, rewardMax = 1):\n",
    "   weights = [1.0] * numActions\n",
    "   t = 0\n",
    "   while True:\n",
    "      probabilityDistribution = distr(weights, gamma)\n",
    "      choice = draw(probabilityDistribution)\n",
    "\n",
    "      theReward = reward(choice, t)\n",
    "      scaledReward = (theReward - rewardMin) / (rewardMax - rewardMin)\n",
    "\n",
    "      estimatedReward = 1.0 * scaledReward / probabilityDistribution[choice]\n",
    "      weights[choice] *= math.exp(estimatedReward * gamma / numActions)\n",
    "\n",
    "      yield theReward\n",
    "      t = t + 1\n",
    "\n",
    "def one_round_invest():\n",
    "    t = 0\n",
    "    cumulativeReward = 1000\n",
    "    numActions = 2\n",
    "    numRounds = 100\n",
    "\n",
    "    rewardVector = [[max(np.random.normal(loc=0.1, scale=0.3**0.5),-1),max(np.random.normal(loc=0.05, scale=0.1**0.5),-1)] for _ in range(numRounds)]\n",
    "    rewards = lambda choice, t: rewardVector[t][choice]\n",
    "\n",
    "    for reward in exp3(numActions, rewards, rewardMin = max(np.array(rewardVector).flatten()), rewardMax = min(np.array(rewardVector).flatten())):\n",
    "        cumulativeReward *= (1+reward)\n",
    "        t += 1\n",
    "        if t >= numRounds:\n",
    "            break\n",
    "    return cumulativeReward\n",
    "\n",
    "for _ in range(100):\n",
    "    reward_history.append(one_round_invest())\n",
    "\n",
    "print(f' The variance of 100 times simulation is: {np.var(reward_history)}')\n",
    "print(f' The mean of 100 times simulation is: {np.mean(reward_history)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Answer:\n",
    "Through the introduction of EXP3 algorithm, the average value of our results is usually higher, but there is also a larger variance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  Implementing Value Iteration\n",
    "The diagram below represents a four period problem, where at each state (a box), you receive reward the number on the box. At each state, you must decide to move upwards or downwards to the next box"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Part 1\n",
    "Consider the diagram above. What is the path(s) that maximizes your total reward (the sum of the values in the boxes)? In your notation, let U denote moving upwards and D denote moving downwards. Then the path U, U, U, D, for example gives reward r = 1 + 0 + 59 − 4 = 56."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Answer:\n",
    "$r=0+(-1)+60+59$, which correspond to D, D, D, U."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Part 2\n",
    "Implement the value iteration algorithm for the problem above (by writing a program) with no more than ten iterations, and report your result. Are you able to achieve the optimal path?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converges at iteration: 9\n",
      "The optimal policy is ['D', 'D', 'D', 'U']\n"
     ]
    }
   ],
   "source": [
    "env={0:[0],1:[1,0],2:[0,3,2,-1],3:[59,36,30,40,60],4:[30,-4,59,30,59,29]}\n",
    "\n",
    "def state_value(env, period:int, idx:int):\n",
    "    next_state = []\n",
    "    if period == 1:\n",
    "        for act1 in range(1,3):\n",
    "            next_state.append(env[period+1][2*(idx+1)-act1])\n",
    "    else:\n",
    "        for act2 in range(2):\n",
    "            next_state.append(env[period+1][(idx+1)-act2])\n",
    "    return max(next_state)\n",
    "\n",
    "\n",
    "def one_iter(env):\n",
    "    converge = False\n",
    "    current_state = []\n",
    "    updated_state = []\n",
    "    for i in range(0,4):\n",
    "        current_state +=env[i]\n",
    "\n",
    "    for i in range(4):\n",
    "        period = i\n",
    "        for idx in range(len(env[period])):\n",
    "            env[period][idx]=0.5*(0.5*state_value(env,period,idx)+env[period][idx])\n",
    "\n",
    "    for i in range(0,4):\n",
    "        updated_state +=env[i]\n",
    "    loss = np.sum(np.fabs(np.array(current_state)-np.array(updated_state)))\n",
    "    if np.sum(np.fabs(np.array(current_state)-np.array(updated_state)))<=1:\n",
    "        converge = True\n",
    "    return env, converge, loss\n",
    "\n",
    "def select_path(env):\n",
    "    policy = []\n",
    "    idx = 0\n",
    "    for i in range(1,5):\n",
    "        if i > 2:\n",
    "            p = np.argmax(env[i][idx:idx+2])\n",
    "            idx = idx + p\n",
    "            policy.append(p)\n",
    "        else:\n",
    "            p = np.argmax(env[i][(idx+1)*2-2:(idx+1)*2])\n",
    "            targe = env[i][(idx+1)*2-2:(idx+1)*2]\n",
    "            idx = int(np.argwhere(np.array(env[i])==targe[p]))\n",
    "            policy.append(p)\n",
    "    return policy\n",
    "\n",
    "def convert(policy:list):\n",
    "    Policy = []\n",
    "    for P in policy:\n",
    "        if P==1:\n",
    "            Policy.append('D')\n",
    "        else:\n",
    "            Policy.append('U')\n",
    "    return Policy\n",
    "\n",
    "\n",
    "\n",
    "iter = 0\n",
    "while True:\n",
    "    iter+=1\n",
    "    env, conv, loss = one_iter(env)\n",
    "    if conv:\n",
    "        print(f'Model converges at iteration: {iter}')\n",
    "        break\n",
    "\n",
    "policy = select_path(env)\n",
    "\n",
    "print(f\"The optimal policy is {convert(policy)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Answer:\n",
    "We can find the optimal path at the $10^th$ iteration, which is D, D, D, U. That is $Start\\rightarrow 0\\rightarrow -1 \\rightarrow 60 \\rightarrow 59$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}